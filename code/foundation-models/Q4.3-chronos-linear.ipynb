{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 20:09:11.293027: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-07 20:09:13.710006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744049354.534865   86008 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744049354.776279   86008 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744049356.740272   86008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744049356.740304   86008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744049356.740305   86008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744049356.740306   86008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-07 20:09:16.951989: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import torch\n",
    "from chronos import ChronosPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = pd.read_parquet('../../data/set-a.parquet').drop(columns=['ICUType']).sort_values(by=['RecordID','Time'])\n",
    "df_b = pd.read_parquet('../../data/set-b.parquet').drop(columns=['ICUType']).sort_values(by=['RecordID','Time'])\n",
    "df_c = pd.read_parquet('../../data/set-c.parquet').drop(columns=['ICUType']).sort_values(by=['RecordID','Time'])\n",
    "\n",
    "df_a = df_a.drop(columns=['Time'])\n",
    "df_b = df_b.drop(columns=['Time'])\n",
    "df_c = df_c.drop(columns=['Time'])\n",
    "\n",
    "\n",
    "outcomes_a = pd.read_csv('../../data/Outcomes-a.txt').sort_values(by=['RecordID']).set_index(\"RecordID\")\n",
    "outcomes_b = pd.read_csv('../../data/Outcomes-b.txt').sort_values(by=['RecordID']).set_index(\"RecordID\")\n",
    "outcomes_c = pd.read_csv('../../data/Outcomes-c.txt').sort_values(by=['RecordID']).set_index(\"RecordID\")\n",
    "\n",
    "outcomes_a = outcomes_a[\"In-hospital_death\"]\n",
    "outcomes_b = outcomes_b[\"In-hospital_death\"]\n",
    "outcomes_c = outcomes_c[\"In-hospital_death\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, outcomes):\n",
    "\n",
    "\n",
    "    # Prepare a list to hold the averaged embeddings for each dataframe\n",
    "    averaged_embeddings = []\n",
    "    labels = []\n",
    "\n",
    "\n",
    "    # Group by 'Category' and iterate over each group\n",
    "    for record_id, group in df.groupby('RecordID'):\n",
    "        # Initialize a list to store the embeddings for each feature (column)\n",
    "        feature_embeddings = []\n",
    "        labels.append(outcomes[record_id]) \n",
    "\n",
    "        # Iterate over each column (feature) in the dataframe\n",
    "        for column in group.columns:\n",
    "            # Get the 1D tensor (a single column) for the feature\n",
    "            context = torch.tensor(group[column].values, dtype=torch.float32)  # Shape: [49]\n",
    "\n",
    "            # Compute the embedding for this column (feature)\n",
    "            embeddings, _ = pipeline.embed(context)\n",
    "        \n",
    "            # We get an embeding for each timestep so we average over all timesteps\n",
    "            embeddings = embeddings.squeeze().mean(axis=0)\n",
    "\n",
    "            # Append the embedding for this feature to the list\n",
    "            feature_embeddings.append(embeddings.detach().cpu().numpy())\n",
    "\n",
    "        # Average embeddings across all features (columns) in this dataframe\n",
    "        averaged_embedding = np.mean(feature_embeddings, axis=0)  # Averaging across all feature embeddings\n",
    "        averaged_embeddings.append(averaged_embedding)\n",
    "        \n",
    "\n",
    "    return np.array(averaged_embeddings), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_dataset(df_a, outcomes_a)\n",
    "X_test, y_test = create_dataset(df_c, outcomes_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "auroc = roc_auc_score(y_test, probs)\n",
    "auprc = average_precision_score(y_test, probs)\n",
    "\n",
    "print(f\"Logistic Regression - AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
