{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d89cdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CUDA\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using device: MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using device: CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device: CPU\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "916d3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)  # Sets the seed for CPU operations\n",
    "    torch.cuda.manual_seed(seed)  # Sets the seed for CUDA GPU operations\n",
    "    torch.cuda.manual_seed_all(seed)  # If using multiple GPUs\n",
    "    random.seed(seed)  # Python's random library\n",
    "    np.random.seed(seed)  # NumPy\n",
    "    \n",
    "    # For determinism in certain CUDA operations\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12f236cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(688)\n",
    "\n",
    "expected_columns = [\n",
    "    'Age', 'Gender', 'Height', 'Weight', 'Albumin', 'ALP', 'ALT', 'AST',\n",
    "    'Bilirubin', 'BUN', 'Cholesterol', 'Creatinine', 'FiO2', 'DiasABP',\n",
    "    'GCS', 'Glucose', 'HCO3', 'HCT', 'HR', 'K', 'Lactate', 'Mg', 'MAP',\n",
    "    'MechVent', 'Na', 'NIDiasABP', 'NIMAP', 'NISysABP', 'PaCO2', 'PaO2',\n",
    "    'pH', 'Platelets', 'RespRate', 'SaO2', 'SysABP', 'Temp', 'TroponinI',\n",
    "    'TroponinT', 'Urine', 'WBC', 'RecordID'\n",
    "]\n",
    "\n",
    "# Load the data\n",
    "outcomes_df_a = pd.read_csv('../../data/Outcomes-a.txt')\n",
    "outcomes_df_a.set_index('RecordID', inplace=True)\n",
    "\n",
    "outcomes_df_b = pd.read_csv('../../data/Outcomes-b.txt')\n",
    "outcomes_df_b.set_index('RecordID', inplace=True)\n",
    "\n",
    "outcomes_df_c = pd.read_csv('../../data/Outcomes-c.txt')\n",
    "outcomes_df_c.set_index('RecordID', inplace=True)\n",
    "\n",
    "\n",
    "# Define static parameters\n",
    "static_params = ['Age','Gender','Height', 'Weight', 'RecordID']\n",
    "\n",
    "# Load the parquet file\n",
    "# 49 rows per patient with timestamps 0 to 48 ( 49 rows per patient)\n",
    "df_a = pd.read_parquet(\"../../data/set-a-imputed-scaled.parquet\")\n",
    "df_a['Time'] = df_a['Time'].str[:2].astype(float)\n",
    "df_b = pd.read_parquet(\"../../data/set-b-imputed-scaled.parquet\")\n",
    "df_b['Time'] = df_b['Time'].str[:2].astype(float)\n",
    "df_c = pd.read_parquet(\"../../data/set-c-imputed-scaled.parquet\")\n",
    "df_c['Time'] = df_c['Time'].str[:2].astype(float)\n",
    "\n",
    "# Merge labels into df_a\n",
    "df_a = df_a.merge(outcomes_df_a, on='RecordID')\n",
    "df_b = df_b.merge(outcomes_df_b, on='RecordID')\n",
    "df_c = df_c.merge(outcomes_df_c, on='RecordID')\n",
    "\n",
    "# Group data by RecordID to create sequences\n",
    "grouped_a = df_a.groupby('RecordID')\n",
    "grouped_b = df_b.groupby('RecordID')\n",
    "grouped_c = df_c.groupby('RecordID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b74b8bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training sample: tensor([ 1.0000e+00,  0.0000e+00, -5.8361e-01, -1.7973e-16,  6.5631e-14,\n",
      "         9.3959e-16, -5.7303e-16,  8.7475e-16, -3.0446e-14, -8.2541e-16,\n",
      "         1.0968e-12, -5.0793e-13, -6.3809e-13, -3.2301e-14,  3.3457e-14,\n",
      "        -7.3069e-15, -7.0036e-15,  2.8909e-16,  1.2499e-15,  5.4436e-13,\n",
      "         1.5882e-16,  2.7398e-16, -1.2818e-13, -1.2364e-15, -2.2249e-16,\n",
      "        -2.0550e-16,  0.0000e+00,  0.0000e+00,  2.5846e-15, -1.6663e-15,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         4.0089e-12,  0.0000e+00,  1.1195e-14, -8.8764e-16,  3.9742e-15,\n",
      "         0.0000e+00])\n",
      "First validation sample: tensor([ 1.0000e+00,  1.0000e+00,  3.2761e-01, -5.4593e-03, -2.9277e-02,\n",
      "         2.2058e-02,  5.4288e-03,  8.7770e-03, -9.5059e-03, -1.0698e-03,\n",
      "         7.1603e-01, -7.1664e-03, -2.3243e-02,  4.2705e-03,  1.8854e-02,\n",
      "         3.6200e-02,  3.9522e-02, -2.0674e-02, -1.3409e-02, -1.9250e-02,\n",
      "        -1.5094e-03,  1.2054e-02,  1.0751e-01, -1.2245e-01,  1.0524e-02,\n",
      "         3.6459e-02, -1.5221e-02, -2.8552e-02, -3.0664e-01,  1.4875e-02,\n",
      "         9.1279e-04,  4.8734e-02,  2.1306e-02, -1.5088e-02, -7.3463e-04,\n",
      "         3.9111e-02,  1.4789e-02,  4.5505e-02,  8.8121e-01, -1.3554e-01,\n",
      "         1.5554e-01])\n",
      "Total sequences in data_a: 4000\n",
      "Total sequences in data_b: 4000\n",
      "Total sequences in data_c: 4000\n"
     ]
    }
   ],
   "source": [
    "# Prepare data and labels for the Transformer\n",
    "data_a = []\n",
    "labels_a = []\n",
    "for record_id, group in grouped_a:\n",
    "    group = group.sort_values(by='Time')  # Ensure rows are sorted by time\n",
    "    # print(f\"RecordID {record_id}: Columns in group = {group.columns.tolist()}, Number of columns = {len(group.columns.tolist())}\")\n",
    "    \n",
    "    features = group.drop(columns=['RecordID', 'Time', 'SAPS-I','SOFA','Length_of_stay','Survival', 'In-hospital_death']).values\n",
    "    \n",
    "    label = group['In-hospital_death'].iloc[0]\n",
    "    # Debug: Print the shape and first few rows of the features\n",
    "    # print(f\"RecordID {record_id}: Features shape = {features.shape}\")\n",
    "    # print(f\"RecordID {record_id}: First row of features = {features[0]}\")\n",
    "    data_a.append(features)  # Drop non-feature columns\n",
    "    labels_a.append(label)\n",
    "\n",
    "\n",
    "    # print(f\"RecordID {record_id}:\")\n",
    "    # print(\"Features shape:\", features.shape)\n",
    "    # print(\"First row of features:\", features[0])  # Print the first row of features\n",
    "    # print(\"Label:\", label)\n",
    "    # Debug: Print the shape of each patient's sequence\n",
    "    # print(f\"RecordID {record_id}: features shape = {features.shape}, label = {label}\")\n",
    "\n",
    "# print(\"Feature columns in training data:\", df_a.drop(columns=['RecordID', 'Time', 'SAPS-I','SOFA','Length_of_stay','Survival', 'In-hospital_death']).columns.tolist())\n",
    "# Convert data and labels to PyTorch tensors\n",
    "data_a = torch.tensor(data_a, dtype=torch.float32)  # Shape: (num_patients, 49, 45)\n",
    "labels_a = torch.tensor(labels_a, dtype=torch.float32)  # Shape: (num_patients,)\n",
    "\n",
    "# Debug: Print the shapes of the tensors\n",
    "\n",
    "\n",
    "data_b = []\n",
    "labels_b = []\n",
    "for record_id, group in grouped_b:\n",
    "    group = group.sort_values(by='Time')  # Ensure rows are sorted by time\n",
    "    features = group.drop(columns=['RecordID', 'Time', 'SAPS-I','SOFA','Length_of_stay','Survival', 'In-hospital_death']).values\n",
    "    label = group['In-hospital_death'].iloc[0]\n",
    "    data_b.append(features)  # Drop non-feature columns\n",
    "    labels_b.append(label)\n",
    "\n",
    "    # Debug: Print the shape of each patient's sequence\n",
    "    # print(f\"RecordID {record_id}: features shape = {features.shape}, label = {label}\")\n",
    "# Convert data and labels to PyTorch tensors\n",
    "data_b = torch.tensor(data_b, dtype=torch.float32)  # Shape: (num_patients, 49, 45)\n",
    "labels_b = torch.tensor(labels_b, dtype=torch.float32)  # Shape: (num_patients,)\n",
    "\n",
    "\n",
    "data_c = []\n",
    "labels_c = []\n",
    "for record_id, group in grouped_c:\n",
    "    group = group.sort_values(by='Time')  # Ensure rows are sorted by time\n",
    "    features = group.drop(columns=['RecordID', 'Time', 'SAPS-I','SOFA','Length_of_stay','Survival', 'In-hospital_death']).values\n",
    "    label = group['In-hospital_death'].iloc[0]\n",
    "    data_c.append(features)  # Drop non-feature columns\n",
    "    labels_c.append(label)\n",
    "\n",
    "    # Debug: Print the shape of each patient's sequence\n",
    "    # print(f\"RecordID {record_id}: features shape = {features.shape}, label = {label}\")\n",
    "# Convert data and labels to PyTorch tensors\n",
    "data_c = torch.tensor(data_c, dtype=torch.float32)  # Shape: (num_patients, 49, 45)\n",
    "labels_c = torch.tensor(labels_c, dtype=torch.float32)  # Shape: (num_patients,)\n",
    "\n",
    "print(\"First training sample:\", data_a[0][0])\n",
    "print(\"First validation sample:\", data_b[0][0])\n",
    "print(f\"Total sequences in data_a: {len(data_a)}\")\n",
    "print(f\"Total sequences in data_b: {len(data_b)}\")\n",
    "print(f\"Total sequences in data_c: {len(data_c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa86ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 100, d_model))  # Adjust max sequence length as needed\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, d_model)\n",
    "        x = self.transformer(x, x)\n",
    "        x = x.mean(dim=0)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90db6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0256e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 41  # Number of features in time series\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "num_layers = 3\n",
    "num_classes = 1  # Binary classification (True/False)\n",
    "learning_rate = 0.00005\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "weight_decay = 0.00016\n",
    "\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = TimeSeriesDataset(data_a, labels_a)\n",
    "validate_dataset = TimeSeriesDataset(data_b, labels_b)\n",
    "test_dataset = TimeSeriesDataset(data_c, labels_c)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d469a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Elias/OneDrive - ETH Zurich/2025SS/Machine Learning for Healthcare/MLH4-Project-1/testvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.3449\n",
      "Validation Loss: 0.3319, Validation Accuracy: 0.8665\n",
      "Validation AuROC: 0.8231, Validation AuPRC: 0.4456\n",
      "New best model found at epoch 1 with Validation AuROC: 0.8231\n",
      "Epoch [2/10], Training Loss: 0.3186\n",
      "Validation Loss: 0.3225, Validation Accuracy: 0.8590\n",
      "Validation AuROC: 0.8359, Validation AuPRC: 0.4636\n",
      "New best model found at epoch 2 with Validation AuROC: 0.8359\n",
      "Epoch [3/10], Training Loss: 0.2992\n",
      "Validation Loss: 0.3227, Validation Accuracy: 0.8680\n",
      "Validation AuROC: 0.8225, Validation AuPRC: 0.4631\n",
      "Epoch [4/10], Training Loss: 0.2904\n",
      "Validation Loss: 0.3325, Validation Accuracy: 0.8708\n",
      "Validation AuROC: 0.8197, Validation AuPRC: 0.4534\n",
      "Epoch [5/10], Training Loss: 0.2733\n",
      "Validation Loss: 0.3239, Validation Accuracy: 0.8662\n",
      "Validation AuROC: 0.8224, Validation AuPRC: 0.4565\n",
      "Epoch [6/10], Training Loss: 0.2624\n",
      "Validation Loss: 0.3288, Validation Accuracy: 0.8632\n",
      "Validation AuROC: 0.8197, Validation AuPRC: 0.4414\n",
      "Epoch [7/10], Training Loss: 0.2520\n",
      "Validation Loss: 0.3662, Validation Accuracy: 0.8490\n",
      "Validation AuROC: 0.8145, Validation AuPRC: 0.4388\n",
      "Epoch [8/10], Training Loss: 0.2411\n",
      "Validation Loss: 0.3490, Validation Accuracy: 0.8622\n",
      "Validation AuROC: 0.8070, Validation AuPRC: 0.4218\n",
      "Epoch [9/10], Training Loss: 0.2229\n",
      "Validation Loss: 0.3831, Validation Accuracy: 0.8595\n",
      "Validation AuROC: 0.7932, Validation AuPRC: 0.4071\n",
      "Epoch [10/10], Training Loss: 0.2152\n",
      "Validation Loss: 0.4101, Validation Accuracy: 0.8535\n",
      "Validation AuROC: 0.7980, Validation AuPRC: 0.4079\n",
      "Best model saved with Validation AuROC: 0.8359\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Transformer model\n",
    "model = TransformerModel(\n",
    "    input_dim=41, \n",
    "    d_model=d_model, \n",
    "    nhead=nhead, \n",
    "    num_layers=num_layers, \n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "best_auroc = 0.0  # Track the best AuROC on the validation set\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Print actual vs. predicted labels\n",
    "        predictions = torch.sigmoid(outputs).squeeze() > 0.5  # Convert logits to binary predictions\n",
    "        # print(f\"Batch {batch_idx}:\")\n",
    "        # print(f\"Actual Labels   : {targets.cpu().int().tolist()}\")\n",
    "        # print(f\"Predicted Labels: {predictions.cpu().int().tolist()}\")\n",
    "\n",
    "\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Average training loss for the epoch\n",
    "    train_loss /= len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_val_targets = []\n",
    "    all_val_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in validate_dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Store predictions and targets for metrics\n",
    "            probabilities = torch.sigmoid(outputs).squeeze()  # Convert logits to probabilities\n",
    "            all_val_predictions.extend(probabilities.cpu().tolist())  # Store probabilities\n",
    "            all_val_targets.extend(targets.cpu().tolist())  # Store actual labels\n",
    "\n",
    "            # Calculate binary predictions for accuracy\n",
    "            predictions = probabilities > 0.5  # Binary classification threshold\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "    # Calculate validation metrics\n",
    "    avg_val_loss = val_loss / len(validate_dataloader)\n",
    "    val_accuracy = correct / total\n",
    "    val_auroc = roc_auc_score(all_val_targets, all_val_predictions)\n",
    "    val_auprc = average_precision_score(all_val_targets, all_val_predictions)\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation AuROC: {val_auroc:.4f}, Validation AuPRC: {val_auprc:.4f}\")\n",
    "\n",
    "    # Save the best model based on validation AuROC\n",
    "    if val_auroc > best_auroc:\n",
    "        best_auroc = val_auroc\n",
    "        best_model_state = model.state_dict()  # Save the model's state\n",
    "        print(f\"New best model found at epoch {epoch+1} with Validation AuROC: {val_auroc:.4f}\")\n",
    "\n",
    "# Save the best model to a file\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, \"best_model.pth\")\n",
    "    print(f\"Best model saved with Validation AuROC: {best_auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9c147fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on the test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19080/3415488116.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))  # Load the best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4326, Test Accuracy: 0.8485\n",
      "Test AuROC: 0.7884, Test AuPRC: 0.4115\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on the test dataset\n",
    "print(\"\\nEvaluating on the test dataset...\")\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))  # Load the best model\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_test_targets = []\n",
    "all_test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Store predictions and targets for metrics\n",
    "        probabilities = torch.sigmoid(outputs).squeeze()  # Convert logits to probabilities\n",
    "        all_test_predictions.extend(probabilities.cpu().tolist())  # Store probabilities\n",
    "        all_test_targets.extend(targets.cpu().tolist())  # Store actual labels\n",
    "\n",
    "        # Calculate binary predictions for accuracy\n",
    "        predictions = probabilities > 0.5  # Binary classification threshold\n",
    "        correct += (predictions == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "# Calculate test metrics\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "test_accuracy = correct / total\n",
    "test_auroc = roc_auc_score(all_test_targets, all_test_predictions)\n",
    "test_auprc = average_precision_score(all_test_targets, all_test_predictions)\n",
    "\n",
    "# Print final test performance\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test AuROC: {test_auroc:.4f}, Test AuPRC: {test_auprc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
